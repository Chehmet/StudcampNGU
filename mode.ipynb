{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06afd5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset pii-masking-400k (C:/Users/Chulpan/.cache/huggingface/datasets/ai4privacy___pii-masking-400k/default/0.0.0/6e80a5852d659186493197b64790257bb60eea22)\n",
      "Downloading and preparing dataset pii-masking-400k/default (download: 491.04 MiB, generated: 421.79 MiB, post-processed: Unknown size, total: 912.84 MiB) to C:/Users/Chulpan/.cache/huggingface/datasets/ai4privacy___pii-masking-400k/default/0.0.0/6e80a5852d659186493197b64790257bb60eea22...\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "Generating train split: 100%|██████████| 325517/325517 [00:04<00:00, 77836.70 examples/s]\n",
      "Generating validation split\n",
      "Generating validation split: 100%|██████████| 81379/81379 [00:00<00:00, 134962.66 examples/s]\n",
      "All the splits matched successfully.\n",
      "Dataset pii-masking-400k downloaded and prepared to C:/Users/Chulpan/.cache/huggingface/datasets/ai4privacy___pii-masking-400k/default/0.0.0/6e80a5852d659186493197b64790257bb60eea22. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"ai4privacy/pii-masking-400k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c658667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (21.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ad10c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка: Директория не найдена по пути 'pii-masking-400k-validation.arrow'\n",
      "Убедись, что указан путь к папке, а не к файлу .arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "\n",
    "# Укажи ПУТЬ К ПАПКЕ, где лежит датасет, а не к самому файлу!\n",
    "dataset_path = 'pii-masking-400k-validation.arrow' # Например, 'pii-masking-400k-validation'\n",
    "\n",
    "try:\n",
    "    # Загружаем весь датасет с диска\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "\n",
    "    print(\"Датасет успешно загружен!\")\n",
    "    print(dataset)\n",
    "\n",
    "    # Теперь ты можешь работать с ним. Например, посмотреть первый элемент:\n",
    "    print(\"\\nПервый пример из датасета:\")\n",
    "    print(dataset[0])\n",
    "\n",
    "    # Если ты хочешь преобразовать его в привычный Pandas DataFrame для анализа:\n",
    "    # Внимание: это может занять много памяти, если датасет большой!\n",
    "    # df = dataset.to_pandas()\n",
    "    # print(\"\\nПервые 5 строк в формате Pandas:\")\n",
    "    # print(df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Ошибка: Директория не найдена по пути '{dataset_path}'\")\n",
    "    print(\"Убедись, что указан путь к папке, а не к файлу .arrow\")\n",
    "except Exception as e:\n",
    "    print(f\"Произошла ошибка при загрузке датасета: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e4a790e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаю обработку файла: synthetic_logs.csv\n",
      "--------------------------------------------------\n",
      "Обработка завершена.\n",
      "Сохранено в файл: piiranha_formatted_logs.jsonl\n",
      "Всего создано записей: 2417\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "TYPE_TO_LABEL = {\n",
    "    'full_name': 'NAME', 'personName': 'NAME', 'givenName': 'NAME', 'contactName': 'NAME', 'user_fullname': 'NAME',\n",
    "    'email': 'EMAIL', 'userMail': 'EMAIL', 'contactMail': 'EMAIL', 'eMail': 'EMAIL', 'emailAddr': 'EMAIL', 'mailbox': 'EMAIL',\n",
    "    'phone': 'TELEPHONENUM', 'contact_phone': 'TELEPHONENUM', 'mobilePhone': 'TELEPHONENUM', 'cellPhone': 'TELEPHONENUM',\n",
    "    'telephone': 'TELEPHONENUM', 'contactNumber': 'TELEPHONENUM', 'mobileNumber': 'TELEPHONENUM', 'phoneNumber': 'TELEPHONENUM',\n",
    "    'credit_card': 'CREDITCARDNUMBER', 'card_number': 'CREDITCARDNUMBER', 'bankCard': 'CREDITCARDNUMBER', 'cardNo': 'CREDITCARDNUMBER',\n",
    "    'cardDetails': 'CREDITCARDNUMBER', 'ccInfo': 'CREDITCARDNUMBER',\n",
    "    'password': 'PASSWORD', 'pass': 'PASSWORD', 'passphrase': 'PASSWORD', 'pwd': 'PASSWORD', 'loginPass': 'PASSWORD',\n",
    "    'secret_key': 'PASSWORD', 'credential': 'PASSWORD', 'accessKey': 'PASSWORD',\n",
    "    'address': 'ADDRESS', 'deliveryAddress': 'ADDRESS', 'home_address': 'ADDRESS', 'physicalAddress': 'ADDRESS',\n",
    "    'billing_address': 'ADDRESS', 'mailingAddress': 'ADDRESS',\n",
    "    'ssn': 'SSN', 'nationalID': 'SSN', 'identityNumber': 'SSN', 'socialSecurityNumber': 'SSN', 'ssNumber': 'SSN',\n",
    "    'tax_id': 'SSN', 'taxIdentification': 'SSN',\n",
    "    'username': 'USERNAME', 'login': 'USERNAME', 'user_id': 'USERNAME', 'accountName': 'USERNAME', 'userTag': 'USERNAME', 'handle': 'USERNAME',\n",
    "    'iban': 'IBAN', 'accountID': 'IBAN', 'bankNo': 'IBAN', 'accountNo': 'IBAN', 'bankID': 'IBAN',\n",
    "    'license_plate': 'PLATENUMBER',\n",
    "    'timestamp': 'DATETIME',\n",
    "    'client_ip': 'IP'\n",
    "}\n",
    "\n",
    "def create_bio_tags(token_offsets, pii_spans_relative):\n",
    "    tags = ['O'] * len(token_offsets)\n",
    "    if not pii_spans_relative: return tags\n",
    "    \n",
    "    for pii in sorted(pii_spans_relative, key=lambda x: x['start']):\n",
    "        pii_start, pii_end = pii['start'], pii['end']\n",
    "        pii_type = TYPE_TO_LABEL.get(pii['type'], 'MISC')\n",
    "        in_entity = False\n",
    "        for i, (token_start, token_end) in enumerate(token_offsets):\n",
    "            if token_start < pii_end and token_end > pii_start:\n",
    "                if tags[i] == 'O':\n",
    "                    if not in_entity:\n",
    "                        tags[i] = f'B-{pii_type}'\n",
    "                        in_entity = True\n",
    "                    else:\n",
    "                        tags[i] = f'I-{pii_type}'\n",
    "    return tags\n",
    "\n",
    "def mask_text(text, pii_spans_relative):\n",
    "    if not pii_spans_relative: return text\n",
    "    \n",
    "    masked_text = list(text)\n",
    "    type_counts = {}\n",
    "    for pii in sorted(pii_spans_relative, key=lambda x: x['start'], reverse=True):\n",
    "        label = TYPE_TO_LABEL.get(pii['type'], 'MISC')\n",
    "        type_counts[label] = type_counts.get(label, 0) + 1\n",
    "        placeholder = f\"[{label}_{type_counts[label]}]\"\n",
    "        start, end = pii['start'], pii['end']\n",
    "        masked_text[start:end] = list(placeholder)\n",
    "    return \"\".join(masked_text)\n",
    "\n",
    "def find_individual_logs(log_block, format_type):\n",
    "    \"\"\"Находит отдельные логи внутри большого блока текста.\"\"\"\n",
    "    if format_type == 'xml':\n",
    "        # Находим все, что заключено в <log>...</log>\n",
    "        return [match for match in re.finditer(r\"<log>.*?</log>\", log_block, re.DOTALL)]\n",
    "    else:\n",
    "        # Для JSON, CSV и text-форматов просто делим по строкам\n",
    "        return [match for match in re.finditer(r\".+\", log_block)]\n",
    "\n",
    "def parse_special_csv(file_content):\n",
    "    \"\"\"Парсер, устойчивый к переносам строк внутри полей.\"\"\"\n",
    "    metadata_pattern = re.compile(r',\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,(.*?),\\s*(\\w+)\\s*$', re.MULTILINE)\n",
    "    header, *rest_of_file = file_content.split('\\n', 1)\n",
    "    content = rest_of_file[0] if rest_of_file else \"\"\n",
    "    \n",
    "    matches = list(metadata_pattern.finditer(content))\n",
    "    last_pos = 0\n",
    "    for match in matches:\n",
    "        log_text_end = match.start()\n",
    "        log_text_block = content[last_pos:log_text_end].strip()\n",
    "        \n",
    "        if log_text_block.startswith('\"') and log_text_block.endswith('\"'):\n",
    "            log_text_block = log_text_block[1:-1].replace('\"\"', '\"')\n",
    "        \n",
    "        _, _, pii_info_str, format_type = match.groups()\n",
    "        if pii_info_str.startswith('\"') and pii_info_str.endswith('\"'):\n",
    "             pii_info_str = pii_info_str[1:-1].replace('\"\"', '\"')\n",
    "        \n",
    "        yield {'log_block': log_text_block, 'pii_info_str': pii_info_str, 'format_type': format_type}\n",
    "        last_pos = match.end()\n",
    "\n",
    "def process_logs_to_piiranha_format(input_csv_path, output_jsonl_path):\n",
    "    print(f\"Начинаю обработку файла: {input_csv_path}\")\n",
    "    total_records_written = 0\n",
    "    \n",
    "    with open(input_csv_path, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_jsonl_path, 'w', encoding='utf-8') as outfile:\n",
    "        \n",
    "        file_content = infile.read()\n",
    "        \n",
    "        for block_idx, row in enumerate(parse_special_csv(file_content)):\n",
    "            log_block = row['log_block']\n",
    "            format_type = row['format_type']\n",
    "            \n",
    "            try:\n",
    "                pii_info_block = json.loads(row['pii_info_str']) if row['pii_info_str'] else []\n",
    "            except json.JSONDecodeError:\n",
    "                pii_info_block = []\n",
    "\n",
    "            # --- Вложенный цикл: обрабатываем каждый лог внутри блока ---\n",
    "            for log_match in find_individual_logs(log_block, format_type):\n",
    "                log_text = log_match.group(0)\n",
    "                log_start_pos = log_match.start()\n",
    "                \n",
    "                # Фильтруем и пересчитываем координаты PII для этого конкретного лога\n",
    "                pii_info_relative = []\n",
    "                for pii in pii_info_block:\n",
    "                    if pii['start'] >= log_start_pos and pii['end'] <= log_match.end():\n",
    "                        pii_info_relative.append({\n",
    "                            **pii,\n",
    "                            'start': pii['start'] - log_start_pos,\n",
    "                            'end': pii['end'] - log_start_pos\n",
    "                        })\n",
    "                \n",
    "                # Стандартная обработка\n",
    "                tokenized = TOKENIZER(log_text, return_offsets_mapping=True, add_special_tokens=False)\n",
    "                bio_tags = create_bio_tags(tokenized['offset_mapping'], pii_info_relative)\n",
    "                masked = mask_text(log_text, pii_info_relative)\n",
    "\n",
    "                output_record = {\n",
    "                    \"source_text\": log_text,\n",
    "                    \"masked_text\": masked,\n",
    "                    \"tokens\": tokenized.tokens(),\n",
    "                    \"ner_labels\": bio_tags,\n",
    "                    \"labels\": [{**pii, \"label\": TYPE_TO_LABEL.get(pii['type'], 'MISC'), \"label_index\": i+1} \n",
    "                               for i, pii in enumerate(pii_info_relative)],\n",
    "                    \"locale\": \"RU\", \"language\": \"ru\", \"split\": \"train\"\n",
    "                }\n",
    "                \n",
    "                outfile.write(json.dumps(output_record, ensure_ascii=False) + '\\n')\n",
    "                total_records_written += 1\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(\"Обработка завершена.\")\n",
    "    print(f\"Сохранено в файл: {output_jsonl_path}\")\n",
    "    print(f\"Всего создано записей: {total_records_written}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"synthetic_logs.csv\" \n",
    "    output_file = \"piiranha_formatted_logs.jsonl\"\n",
    "    process_logs_to_piiranha_format(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56bbc169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасет успешно загружен из кэша!\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source_text', 'locale', 'language', 'split', 'privacy_mask', 'uid', 'masked_text', 'mbert_tokens', 'mbert_token_classes'],\n",
      "        num_rows: 325517\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['source_text', 'locale', 'language', 'split', 'privacy_mask', 'uid', 'masked_text', 'mbert_tokens', 'mbert_token_classes'],\n",
      "        num_rows: 81379\n",
      "    })\n",
      "})\n",
      "\n",
      "--- Первый элемент датасета ---\n",
      "{'source_text': '<p>My child faozzsd379223 (DOB: May/58) will undergo treatment with Dr. faozzsd379223, office at Hill Road. Our ZIP code is 28170-6392. Consult policy M.UE.227995. Contact number: 0070.606.322.6244. Handle transactions with 6225427220412963. Queries? Email: faozzsd379223@outlook.com.</p>', 'locale': 'US', 'language': 'en', 'split': 'train', 'privacy_mask': [{'label': 'USERNAME', 'start': 12, 'end': 25, 'value': 'faozzsd379223', 'label_index': 2}, {'label': 'DATEOFBIRTH', 'start': 32, 'end': 38, 'value': 'May/58', 'label_index': 1}, {'label': 'USERNAME', 'start': 72, 'end': 85, 'value': 'faozzsd379223', 'label_index': 1}, {'label': 'STREET', 'start': 97, 'end': 106, 'value': 'Hill Road', 'label_index': 1}, {'label': 'ZIPCODE', 'start': 124, 'end': 134, 'value': '28170-6392', 'label_index': 1}, {'label': 'TELEPHONENUM', 'start': 180, 'end': 197, 'value': '0070.606.322.6244', 'label_index': 1}, {'label': 'CREDITCARDNUMBER', 'start': 224, 'end': 240, 'value': '6225427220412963', 'label_index': 1}, {'label': 'EMAIL', 'start': 258, 'end': 283, 'value': 'faozzsd379223@outlook.com', 'label_index': 1}], 'uid': 302521, 'masked_text': '<p>My child [USERNAME_2] (DOB: [DATEOFBIRTH_1]) will undergo treatment with Dr. [USERNAME_1], office at [STREET_1]. Our ZIP code is [ZIPCODE_1]. Consult policy M.UE.227995. Contact number: [TELEPHONENUM_1]. Handle transactions with [CREDITCARDNUMBER_1]. Queries? Email: [EMAIL_1].</p>', 'mbert_tokens': ['<', 'p', '>', 'My', 'child', 'fa', '##oz', '##zs', '##d', '##3', '##7', '##9', '##22', '##3', '(', 'DO', '##B', ':', 'May', '/', '58', ')', 'will', 'under', '##go', 'treatment', 'with', 'Dr', '.', 'fa', '##oz', '##zs', '##d', '##3', '##7', '##9', '##22', '##3', ',', 'office', 'at', 'Hill', 'Road', '.', 'Our', 'Z', '##IP', 'code', 'is', '281', '##70', '-', '639', '##2', '.', 'Con', '##sul', '##t', 'policy', 'M', '.', 'UE', '.', '227', '##99', '##5', '.', 'Contact', 'number', ':', '007', '##0', '.', '606', '.', '322', '.', '624', '##4', '.', 'Hand', '##le', 'transaction', '##s', 'with', '622', '##5', '##42', '##7', '##22', '##04', '##12', '##9', '##6', '##3', '.', 'Que', '##ries', '?', 'Em', '##ail', ':', 'fa', '##oz', '##zs', '##d', '##3', '##7', '##9', '##22', '##3', '@', 'out', '##lo', '##ok', '.', 'com', '.', '<', '/', 'p', '>'], 'mbert_token_classes': ['O', 'O', 'O', 'O', 'O', 'B-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'O', 'O', 'O', 'O', 'B-DATEOFBIRTH', 'I-DATEOFBIRTH', 'I-DATEOFBIRTH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'O', 'O', 'O', 'B-STREET', 'I-STREET', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ZIPCODE', 'I-ZIPCODE', 'I-ZIPCODE', 'I-ZIPCODE', 'I-ZIPCODE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TELEPHONENUM', 'I-TELEPHONENUM', 'I-TELEPHONENUM', 'I-TELEPHONENUM', 'I-TELEPHONENUM', 'I-TELEPHONENUM', 'I-TELEPHONENUM', 'I-TELEPHONENUM', 'I-TELEPHONENUM', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'O', 'O', 'O', 'O', 'O']}\n",
      "\n",
      "--- Первые 5 строк в виде таблицы Pandas ---\n",
      "                                         source_text locale language  split  \\\n",
      "0  <p>My child faozzsd379223 (DOB: May/58) will u...     US       en  train   \n",
      "1  Guardians:*BF6* and *BF6* grant permission for...     GB       en  train   \n",
      "2  We, *bahara.cathers19* and *bahara.cathers19* ...     GB       en  train   \n",
      "3  Student: Blagojka van der Boog\\nDOB: 8th Janua...     US       en  train   \n",
      "4  Child: Anna-Louise Dolderer\\nDate of Birth: 05...     US       en  train   \n",
      "\n",
      "                                        privacy_mask     uid  \\\n",
      "0  [{'label': 'USERNAME', 'start': 12, 'end': 25,...  302521   \n",
      "1  [{'label': 'USERNAME', 'start': 11, 'end': 14,...  120409   \n",
      "2  [{'label': 'USERNAME', 'start': 5, 'end': 21, ...  120411   \n",
      "3  [{'label': 'GIVENNAME', 'start': 9, 'end': 17,...  128429   \n",
      "4  [{'label': 'GIVENNAME', 'start': 7, 'end': 18,...  128431   \n",
      "\n",
      "                                         masked_text  \\\n",
      "0  <p>My child [USERNAME_2] (DOB: [DATEOFBIRTH_1]...   \n",
      "1  Guardians:*[USERNAME_4]* and *[USERNAME_3]* gr...   \n",
      "2  We, *[USERNAME_3]* and *[USERNAME_2]* reside a...   \n",
      "3  Student: [GIVENNAME_2] [SURNAME_2]\\nDOB: [DATE...   \n",
      "4  Child: [GIVENNAME_2] [SURNAME_2]\\nDate of Birt...   \n",
      "\n",
      "                                        mbert_tokens  \\\n",
      "0  [<, p, >, My, child, fa, ##oz, ##zs, ##d, ##3,...   \n",
      "1  [Guardian, ##s, :, *, BF, ##6, *, and, *, BF, ...   \n",
      "2  [We, ,, *, ba, ##hara, ., cat, ##hers, ##19, *...   \n",
      "3  [Student, :, B, ##lag, ##oj, ##ka, van, der, B...   \n",
      "4  [Child, :, Anna, -, Louise, Dol, ##dere, ##r, ...   \n",
      "\n",
      "                                 mbert_token_classes  \n",
      "0  [O, O, O, O, O, B-USERNAME, I-USERNAME, I-USER...  \n",
      "1  [O, O, O, O, B-USERNAME, I-USERNAME, O, O, O, ...  \n",
      "2  [O, O, O, B-USERNAME, I-USERNAME, I-USERNAME, ...  \n",
      "3  [O, O, B-GIVENNAME, I-GIVENNAME, I-GIVENNAME, ...  \n",
      "4  [O, O, B-GIVENNAME, I-GIVENNAME, I-GIVENNAME, ...  \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Указываем имя датасета, как на Hugging Face Hub.\n",
    "# Библиотека сама найдет его в твоем кэше и правильно соберет.\n",
    "dataset_name = \"ai4privacy/pii-masking-400k\"\n",
    "\n",
    "try:\n",
    "    # Загружаем датасет (он будет взят из кэша, скачиваться не будет)\n",
    "    dataset = load_dataset(dataset_name)\n",
    "\n",
    "    print(\"Датасет успешно загружен из кэша!\")\n",
    "    print(dataset)\n",
    "\n",
    "    # Выбираем нужную часть (например, validation)\n",
    "    validation_data = dataset['train'] # В этом датасете нет 'validation', только 'train' и 'test'\n",
    "\n",
    "    # Посмотрим на первый пример\n",
    "    print(\"\\n--- Первый элемент датасета ---\")\n",
    "    print(validation_data[0])\n",
    "\n",
    "    # Для удобного анализа можно преобразовать часть данных в Pandas DataFrame\n",
    "    # Внимание: не делай это для всего датасета сразу, 400к строк могут занять всю память!\n",
    "    # Возьмем первые 1000 строк.\n",
    "    df = validation_data.select(range(1000)).to_pandas()\n",
    "\n",
    "    print(\"\\n--- Первые 5 строк в виде таблицы Pandas ---\")\n",
    "    print(df.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Произошла ошибка: {e}\")\n",
    "    print(\"Убедись, что имя датасета верное и кэш не поврежден.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
