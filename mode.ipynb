{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06afd5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset pii-masking-400k (C:/Users/Chulpan/.cache/huggingface/datasets/ai4privacy___pii-masking-400k/default/0.0.0/6e80a5852d659186493197b64790257bb60eea22)\n",
      "Downloading and preparing dataset pii-masking-400k/default (download: 491.04 MiB, generated: 421.79 MiB, post-processed: Unknown size, total: 912.84 MiB) to C:/Users/Chulpan/.cache/huggingface/datasets/ai4privacy___pii-masking-400k/default/0.0.0/6e80a5852d659186493197b64790257bb60eea22...\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "Generating train split: 100%|██████████| 325517/325517 [00:04<00:00, 77836.70 examples/s]\n",
      "Generating validation split\n",
      "Generating validation split: 100%|██████████| 81379/81379 [00:00<00:00, 134962.66 examples/s]\n",
      "All the splits matched successfully.\n",
      "Dataset pii-masking-400k downloaded and prepared to C:/Users/Chulpan/.cache/huggingface/datasets/ai4privacy___pii-masking-400k/default/0.0.0/6e80a5852d659186493197b64790257bb60eea22. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"ai4privacy/pii-masking-400k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c658667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (21.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ad10c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка: Директория не найдена по пути 'pii-masking-400k-validation.arrow'\n",
      "Убедись, что указан путь к папке, а не к файлу .arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "\n",
    "# Укажи ПУТЬ К ПАПКЕ, где лежит датасет, а не к самому файлу!\n",
    "dataset_path = 'pii-masking-400k-validation.arrow' # Например, 'pii-masking-400k-validation'\n",
    "\n",
    "try:\n",
    "    # Загружаем весь датасет с диска\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "\n",
    "    print(\"Датасет успешно загружен!\")\n",
    "    print(dataset)\n",
    "\n",
    "    # Теперь ты можешь работать с ним. Например, посмотреть первый элемент:\n",
    "    print(\"\\nПервый пример из датасета:\")\n",
    "    print(dataset[0])\n",
    "\n",
    "    # Если ты хочешь преобразовать его в привычный Pandas DataFrame для анализа:\n",
    "    # Внимание: это может занять много памяти, если датасет большой!\n",
    "    # df = dataset.to_pandas()\n",
    "    # print(\"\\nПервые 5 строк в формате Pandas:\")\n",
    "    # print(df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Ошибка: Директория не найдена по пути '{dataset_path}'\")\n",
    "    print(\"Убедись, что указан путь к папке, а не к файлу .arrow\")\n",
    "except Exception as e:\n",
    "    print(f\"Произошла ошибка при загрузке датасета: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e4a790e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Шаг 2: Обработка файла 'synthetic_logs.csv' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка блоков логов:   0%|          | 0/200 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Обработка блоков логов: 100%|██████████| 200/200 [00:00<00:00, 218.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Подготовка данных завершена! ---\n",
      "Найдено и обработано PII сущностей: 0\n",
      "Всего создано записей для обучения: 2417\n",
      "Результат сохранен в: 'piiranha_formatted_logs.jsonl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Файл: 1_prepare_data.py (Финальная, исправленная версия 3.0)\n",
    "\n",
    "import json\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- КОНФИГУРАЦИЯ ---\n",
    "input_csv_file = \"synthetic_logs.csv\"\n",
    "output_jsonl_file = \"piiranha_formatted_logs.jsonl\"\n",
    "# Используем тот же токенизатор, что и в оригинальной модели piiranha\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# --- СЛОВАРЬ МЕТОК ---\n",
    "TYPE_TO_LABEL = {\n",
    "    'full_name': 'NAME', 'personName': 'NAME', 'givenName': 'NAME', 'contactName': 'NAME', 'user_fullname': 'NAME',\n",
    "    'email': 'EMAIL', 'userMail': 'EMAIL', 'contactMail': 'EMAIL', 'eMail': 'EMAIL', 'emailAddr': 'EMAIL', 'mailbox': 'EMAIL',\n",
    "    'phone': 'TELEPHONENUM', 'contact_phone': 'TELEPHONENUM', 'mobilePhone': 'TELEPHONENUM', 'cellPhone': 'TELEPHONENUM',\n",
    "    'telephone': 'TELEPHONENUM', 'contactNumber': 'TELEPHONENUM', 'mobileNumber': 'TELEPHONENUM', 'phoneNumber': 'TELEPHONENUM',\n",
    "    'credit_card': 'CREDITCARDNUMBER', 'card_number': 'CREDITCARDNUMBER', 'bankCard': 'CREDITCARDNUMBER', 'cardNo': 'CREDITCARDNUMBER',\n",
    "    'cardDetails': 'CREDITCARDNUMBER', 'ccInfo': 'CREDITCARDNUMBER',\n",
    "    'password': 'PASSWORD', 'pass': 'PASSWORD', 'passphrase': 'PASSWORD', 'pwd': 'PASSWORD', 'loginPass': 'PASSWORD',\n",
    "    'secret_key': 'PASSWORD', 'credential': 'PASSWORD', 'accessKey': 'PASSWORD',\n",
    "    'address': 'ADDRESS', 'deliveryAddress': 'ADDRESS', 'home_address': 'ADDRESS', 'physicalAddress': 'ADDRESS',\n",
    "    'billing_address': 'ADDRESS', 'mailingAddress': 'ADDRESS',\n",
    "    'ssn': 'SSN', 'nationalID': 'SSN', 'identityNumber': 'SSN', 'socialSecurityNumber': 'SSN', 'ssNumber': 'SSN',\n",
    "    'tax_id': 'SSN', 'taxIdentification': 'SSN',\n",
    "    'username': 'USERNAME', 'login': 'USERNAME', 'user_id': 'USERNAME', 'accountName': 'USERNAME', 'userTag': 'USERNAME', 'handle': 'USERNAME',\n",
    "    'iban': 'IBAN', 'accountID': 'IBAN', 'bankNo': 'IBAN', 'accountNo': 'IBAN', 'bankID': 'IBAN',\n",
    "    'license_plate': 'PLATENUMBER', 'timestamp': 'DATETIME', 'client_ip': 'IP'\n",
    "}\n",
    "\n",
    "def parse_special_csv(file_content):\n",
    "    \"\"\"Надежный парсер для сложного CSV.\"\"\"\n",
    "    metadata_pattern = re.compile(r',\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,(.*?),\\s*(\\w+)\\s*$', re.MULTILINE)\n",
    "    header, *rest = file_content.split('\\n', 1)\n",
    "    content = rest[0] if rest else \"\"\n",
    "    last_pos = 0\n",
    "    for match in metadata_pattern.finditer(content):\n",
    "        log_block = content[last_pos:match.start()].strip()\n",
    "        if log_block.startswith('\"') and log_block.endswith('\"'): log_block = log_block[1:-1].replace('\"\"', '\"')\n",
    "        pii_str = match.group(3)\n",
    "        if pii_str.startswith('\"') and pii_str.endswith('\"'): pii_str = pii_str[1:-1].replace('\"\"', '\"')\n",
    "        yield {'log_block': log_block, 'pii_info_str': pii_str, 'format_type': match.group(4)}\n",
    "        last_pos = match.end()\n",
    "\n",
    "def find_individual_logs(log_block, format_type):\n",
    "    \"\"\"Находит отдельные логи внутри блока.\"\"\"\n",
    "    if format_type == 'xml':\n",
    "        return list(re.finditer(r\"<log>.*?</log>\", log_block, re.DOTALL))\n",
    "    else: # JSON, CSV, text\n",
    "        return list(re.finditer(r\".+\", log_block))\n",
    "\n",
    "# --- ОСНОВНОЙ СКРИПТ ---\n",
    "print(f\"--- Шаг 2: Обработка файла '{input_csv_file}' ---\")\n",
    "total_records_written = 0\n",
    "total_pii_found = 0\n",
    "\n",
    "with open(input_csv_file, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_jsonl_file, 'w', encoding='utf-8') as outfile:\n",
    "    \n",
    "    file_content = infile.read()\n",
    "    \n",
    "    for block_data in tqdm(list(parse_special_csv(file_content)), desc=\"Обработка блоков логов\"):\n",
    "        log_block = block_data['log_block']\n",
    "        format_type = block_data['format_type']\n",
    "        \n",
    "        try:\n",
    "            pii_info_block = json.loads(block_data['pii_info_str']) if block_data['pii_info_str'] else []\n",
    "        except json.JSONDecodeError:\n",
    "            pii_info_block = []\n",
    "\n",
    "        if not log_block: continue\n",
    "\n",
    "        for log_match in find_individual_logs(log_block, format_type):\n",
    "            log_text = log_match.group(0).strip()\n",
    "            if not log_text: continue\n",
    "            \n",
    "            log_start_char, log_end_char = log_match.span()\n",
    "            \n",
    "            # --- НОВАЯ, БОЛЕЕ НАДЕЖНАЯ ЛОГИКА РАЗМЕТКИ ---\n",
    "            # 1. Токенизируем текст и получаем слова (word_ids)\n",
    "            # ВАЖНО: Мы не можем использовать is_split_into_words=True, т.к. нам нужны оригинальные слова\n",
    "            tokenized = tokenizer(log_text, return_offsets_mapping=True)\n",
    "            tokens = tokenizer.convert_ids_to_tokens(tokenized.input_ids)\n",
    "            word_ids = tokenized.word_ids()\n",
    "\n",
    "            # 2. Создаем \"пустые\" метки\n",
    "            labels = ['O'] * len(tokens)\n",
    "            \n",
    "            # 3. Находим PII, относящиеся к этому логу\n",
    "            for pii in pii_info_block:\n",
    "                if pii['start'] >= log_start_char and pii['end'] <= log_end_char:\n",
    "                    # Пересчитываем координаты относительно начала этого лога\n",
    "                    pii_start_relative = pii['start'] - log_start_char\n",
    "                    pii_end_relative = pii['end'] - log_start_char\n",
    "                    pii_type = TYPE_TO_LABEL.get(pii['type'], 'MISC')\n",
    "                    \n",
    "                    is_first_token_in_pii = True\n",
    "                    for i, (start, end) in enumerate(tokenized.offset_mapping):\n",
    "                        # Проверяем, что токен не является спец. токеном и пересекается с PII\n",
    "                        if start == end: continue # Пропускаем спец. токены типа [CLS], [SEP]\n",
    "                        \n",
    "                        if start < pii_end_relative and end > pii_start_relative:\n",
    "                            if is_first_token_in_pii:\n",
    "                                labels[i] = f'B-{pii_type}'\n",
    "                                is_first_token_in_pii = False\n",
    "                            else:\n",
    "                                labels[i] = f'I-{pii_type}'\n",
    "                    \n",
    "                    if not is_first_token_in_pii: # Если хоть один токен был помечен\n",
    "                        total_pii_found += 1\n",
    "\n",
    "            # 4. Сохраняем результат\n",
    "            output_record = {\n",
    "                \"source_text\": log_text,\n",
    "                \"tokens\": tokens,\n",
    "                \"ner_labels\": labels,\n",
    "            }\n",
    "            outfile.write(json.dumps(output_record, ensure_ascii=False) + '\\n')\n",
    "            total_records_written += 1\n",
    "\n",
    "print(\"\\n--- Подготовка данных завершена! ---\")\n",
    "print(f\"Найдено и обработано PII сущностей: {total_pii_found}\")\n",
    "print(f\"Всего создано записей для обучения: {total_records_written}\")\n",
    "print(f\"Результат сохранен в: '{output_jsonl_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56bbc169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Шаг 1: Загрузка токенизатора и конфига модели ---\n",
      "--- Шаг 2: Обработка файла 'synthetic_logs.csv' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка блоков логов: 100%|██████████| 200/200 [00:00<00:00, 221.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Шаг 3: Преобразование в формат Hugging Face ---\n",
      "\n",
      "--- Шаг 4: Финальная токенизация и выравнивание меток ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Выравнивание меток: 100%|██████████| 2417/2417 [00:01<00:00, 1552.28 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Шаг 5: Сохранение готового датасета в './tokenized_log_dataset' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2417/2417 [00:00<00:00, 92223.18 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ПОДГОТОВКА ДАННЫХ УСПЕШНО ЗАВЕРШЕНА! ---\n",
      "Найдено и обработано PII сущностей: 0\n",
      "Всего создано записей для обучения: 2417\n",
      "Теперь можно запускать скрипт обучения (2_train_model.py)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Файл: prepare_and_tokenize.py (Финальная, рабочая версия)\n",
    "\n",
    "import json\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- КОНФИГУРАЦИЯ ---\n",
    "input_csv_file = \"synthetic_logs.csv\"\n",
    "tokenized_dataset_path = \"./tokenized_log_dataset\"\n",
    "model_name = \"iiiorg/piiranha-v1-detect-personal-information\"\n",
    "\n",
    "# --- ЗАГРУЗКА ---\n",
    "print(\"--- Шаг 1: Загрузка токенизатора и конфига модели ---\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_config = AutoModelForTokenClassification.from_pretrained(model_name).config\n",
    "id2label = {int(k): v for k, v in model_config.id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# --- СЛОВАРЬ МЕТОК ---\n",
    "TYPE_TO_LABEL = {\n",
    "    'full_name': 'NAME', 'personName': 'NAME', 'givenName': 'NAME', 'contactName': 'NAME', 'user_fullname': 'NAME',\n",
    "    'email': 'EMAIL', 'userMail': 'EMAIL', 'contactMail': 'EMAIL', 'eMail': 'EMAIL', 'emailAddr': 'EMAIL', 'mailbox': 'EMAIL',\n",
    "    'phone': 'TELEPHONENUM', 'contact_phone': 'TELEPHONENUM', 'mobilePhone': 'TELEPHONENUM', 'cellPhone': 'TELEPHONENUM',\n",
    "    'telephone': 'TELEPHONENUM', 'contactNumber': 'TELEPHONENUM', 'mobileNumber': 'TELEPHONENUM', 'phoneNumber': 'TELEPHONENUM',\n",
    "    'credit_card': 'CREDITCARDNUMBER', 'card_number': 'CREDITCARDNUMBER', 'bankCard': 'CREDITCARDNUMBER', 'cardNo': 'CREDITCARDNUMBER',\n",
    "    'cardDetails': 'CREDITCARDNUMBER', 'ccInfo': 'CREDITCARDNUMBER',\n",
    "    'password': 'PASSWORD', 'pass': 'PASSWORD', 'passphrase': 'PASSWORD', 'pwd': 'PASSWORD', 'loginPass': 'PASSWORD',\n",
    "    'secret_key': 'PASSWORD', 'credential': 'PASSWORD', 'accessKey': 'PASSWORD',\n",
    "    'address': 'ADDRESS', 'deliveryAddress': 'ADDRESS', 'home_address': 'ADDRESS', 'physicalAddress': 'ADDRESS',\n",
    "    'billing_address': 'ADDRESS', 'mailingAddress': 'ADDRESS',\n",
    "    'ssn': 'SSN', 'nationalID': 'SSN', 'identityNumber': 'SSN', 'socialSecurityNumber': 'SSN', 'ssNumber': 'SSN',\n",
    "    'tax_id': 'SSN', 'taxIdentification': 'SSN',\n",
    "    'username': 'USERNAME', 'login': 'USERNAME', 'user_id': 'USERNAME', 'accountName': 'USERNAME', 'userTag': 'USERNAME', 'handle': 'USERNAME',\n",
    "    'iban': 'IBAN', 'accountID': 'IBAN', 'bankNo': 'IBAN', 'accountNo': 'IBAN', 'bankID': 'IBAN',\n",
    "    'license_plate': 'PLATENUMBER', 'timestamp': 'DATETIME', 'client_ip': 'IP'\n",
    "}\n",
    "\n",
    "def parse_special_csv(file_content):\n",
    "    metadata_pattern = re.compile(r',\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,(.*?),\\s*(\\w+)\\s*$', re.MULTILINE)\n",
    "    header, *rest = file_content.split('\\n', 1)\n",
    "    content = rest[0] if rest else \"\"\n",
    "    last_pos = 0\n",
    "    for match in metadata_pattern.finditer(content):\n",
    "        log_block = content[last_pos:match.start()].strip()\n",
    "        if log_block.startswith('\"') and log_block.endswith('\"'): log_block = log_block[1:-1].replace('\"\"', '\"')\n",
    "        pii_str = match.group(3)\n",
    "        if pii_str.startswith('\"') and pii_str.endswith('\"'): pii_str = pii_str[1:-1].replace('\"\"', '\"')\n",
    "        yield {'log_block': log_block, 'pii_info_str': pii_str, 'format_type': match.group(4)}\n",
    "        last_pos = match.end()\n",
    "\n",
    "def find_individual_logs(log_block, format_type):\n",
    "    if format_type == 'xml':\n",
    "        return list(re.finditer(r\"<log>.*?</log>\", log_block, re.DOTALL))\n",
    "    else:\n",
    "        return list(re.finditer(r\".+\", log_block))\n",
    "\n",
    "# --- ОСНОВНОЙ КОД ОБРАБОТКИ ---\n",
    "print(f\"--- Шаг 2: Обработка файла '{input_csv_file}' ---\")\n",
    "all_records = []\n",
    "total_pii_found = 0\n",
    "\n",
    "with open(input_csv_file, 'r', encoding='utf-8') as infile:\n",
    "    file_content = infile.read()\n",
    "    \n",
    "    for block_data in tqdm(list(parse_special_csv(file_content)), desc=\"Обработка блоков логов\"):\n",
    "        log_block, format_type = block_data['log_block'], block_data['format_type']\n",
    "        try: pii_info_block = json.loads(block_data['pii_info_str']) if block_data['pii_info_str'] else []\n",
    "        except json.JSONDecodeError: pii_info_block = []\n",
    "\n",
    "        if not log_block: continue\n",
    "\n",
    "        for log_match in find_individual_logs(log_block, format_type):\n",
    "            log_text = log_match.group(0).strip()\n",
    "            if not log_text: continue\n",
    "            \n",
    "            log_start_char, log_end_char = log_match.span()\n",
    "            \n",
    "            # --- САМАЯ НАДЕЖНАЯ ЛОГИКА РАЗМЕТКИ ---\n",
    "            tokens = tokenizer.tokenize(log_text)\n",
    "            labels = ['O'] * len(tokens)\n",
    "            \n",
    "            pii_in_log = [p for p in pii_info_block if p['start'] >= log_start_char and p['end'] <= log_end_char]\n",
    "            \n",
    "            if pii_in_log:\n",
    "                tokenization = tokenizer(log_text, return_offsets_mapping=True)\n",
    "                for pii in pii_in_log:\n",
    "                    pii_start = pii['start'] - log_start_char\n",
    "                    pii_end = pii['end'] - log_start_char\n",
    "                    pii_type = TYPE_TO_LABEL.get(pii['type'], 'MISC')\n",
    "                    \n",
    "                    is_first = True\n",
    "                    for i, (start, end) in enumerate(tokenization['offset_mapping']):\n",
    "                        if start == end: continue # Пропускаем спец. токены\n",
    "                        if max(start, pii_start) < min(end, pii_end): # Проверка на пересечение\n",
    "                            if is_first:\n",
    "                                labels[i-1] = f\"B-{pii_type}\" # -1 из-за [CLS] токена\n",
    "                                is_first = False\n",
    "                            else:\n",
    "                                labels[i-1] = f\"I-{pii_type}\"\n",
    "                    if not is_first:\n",
    "                        total_pii_found += 1\n",
    "            \n",
    "            all_records.append({\"tokens\": tokens, \"ner_tags\": labels})\n",
    "\n",
    "# --- ПРЕОБРАЗОВАНИЕ В ДАТАСЕТ И ТОКЕНИЗАЦИЯ ДЛЯ МОДЕЛИ ---\n",
    "print(\"\\n--- Шаг 3: Преобразование в формат Hugging Face ---\")\n",
    "dataset = Dataset.from_list(all_records)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, tags in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                tag = tags[word_idx]\n",
    "                label_ids.append(label2id.get(tag, label2id[\"O\"]))\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "print(\"\\n--- Шаг 4: Финальная токенизация и выравнивание меток ---\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Выравнивание меток\"\n",
    ")\n",
    "\n",
    "# --- СОХРАНЕНИЕ ---\n",
    "print(f\"\\n--- Шаг 5: Сохранение готового датасета в '{tokenized_dataset_path}' ---\")\n",
    "tokenized_dataset.save_to_disk(tokenized_dataset_path)\n",
    "\n",
    "print(\"\\n--- ПОДГОТОВКА ДАННЫХ УСПЕШНО ЗАВЕРШЕНА! ---\")\n",
    "print(f\"Найдено и обработано PII сущностей: {total_pii_found}\")\n",
    "print(f\"Всего создано записей для обучения: {len(tokenized_dataset)}\")\n",
    "print(\"Теперь можно запускать скрипт обучения (2_train_model.py)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29974f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ячейка 1: Настройка завершена.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- КОНФИГУРАЦИЯ ---\n",
    "input_csv_file = \"synthetic_logs.csv\"\n",
    "tokenized_dataset_path = \"./tokenized_log_dataset\"\n",
    "model_name = \"iiiorg/piiranha-v1-detect-personal-information\"\n",
    "\n",
    "# --- СЛОВАРЬ МЕТОК (самый полный) ---\n",
    "TYPE_TO_LABEL = {\n",
    "    'full_name': 'NAME', 'personName': 'NAME', 'givenName': 'NAME', 'contactName': 'NAME', 'user_fullname': 'NAME',\n",
    "    'email': 'EMAIL', 'userMail': 'EMAIL', 'contactMail': 'EMAIL', 'eMail': 'EMAIL', 'emailAddr': 'EMAIL', 'mailbox': 'EMAIL',\n",
    "    'phone': 'TELEPHONENUM', 'contact_phone': 'TELEPHONENUM', 'mobilePhone': 'TELEPHONENUM', 'cellPhone': 'TELEPHONENUM',\n",
    "    'telephone': 'TELEPHONENUM', 'contactNumber': 'TELEPHONENUM', 'mobileNumber': 'TELEPHONENUM', 'phoneNumber': 'TELEPHONENUM', 'tel': 'TELEPHONENUM',\n",
    "    'credit_card': 'CREDITCARDNUMBER', 'card_number': 'CREDITCARDNUMBER', 'bankCard': 'CREDITCARDNUMBER', 'cardNo': 'CREDITCARDNUMBER',\n",
    "    'cardDetails': 'CREDITCARDNUMBER', 'ccInfo': 'CREDITCARDNUMBER', 'payment_card': 'CREDITCARDNUMBER', 'paymentCard': 'CREDITCARDNUMBER', 'ccNumber': 'CREDITCARDNUMBER',\n",
    "    'password': 'PASSWORD', 'pass': 'PASSWORD', 'passphrase': 'PASSWORD', 'pwd': 'PASSWORD', 'loginPass': 'PASSWORD',\n",
    "    'secret_key': 'PASSWORD', 'credential': 'PASSWORD', 'accessKey': 'PASSWORD', 'secretCode': 'PASSWORD', 'authToken': 'PASSWORD', 'passKey': 'PASSWORD',\n",
    "    'address': 'ADDRESS', 'deliveryAddress': 'ADDRESS', 'home_address': 'ADDRESS', 'physicalAddress': 'ADDRESS',\n",
    "    'billing_address': 'ADDRESS', 'mailingAddress': 'ADDRESS', 'location': 'ADDRESS', 'residence': 'ADDRESS', 'street': 'ADDRESS', 'postalAddress': 'ADDRESS',\n",
    "    'ssn': 'SSN', 'nationalID': 'SSN', 'identityNumber': 'SSN', 'socialSecurityNumber': 'SSN', 'ssNumber': 'SSN',\n",
    "    'tax_id': 'SSN', 'taxIdentification': 'SSN', 'taxNumber': 'SSN', 'personalID': 'SSN', 'socialID': 'SSN',\n",
    "    'username': 'USERNAME', 'login': 'USERNAME', 'user_id': 'USERNAME', 'accountName': 'USERNAME', 'userTag': 'USERNAME', 'handle': 'USERNAME',\n",
    "    'userLogin': 'USERNAME', 'userIdentifier': 'USERNAME', 'nickname': 'USERNAME', 'screenName': 'USERNAME', 'loginID': 'USERNAME', 'userName': 'USERNAME',\n",
    "    'iban': 'IBAN', 'accountID': 'IBAN', 'bankNo': 'IBAN', 'accountNo': 'IBAN', 'bankID': 'IBAN', 'financialAccount': 'IBAN',\n",
    "    'account_number': 'IBAN', 'bank_account': 'IBAN', 'accountCode': 'IBAN', 'bankCode': 'IBAN', 'bankAccountNumber': 'IBAN',\n",
    "    'license_plate': 'PLATENUMBER', 'timestamp': 'DATETIME', 'client_ip': 'IP'\n",
    "}\n",
    "\n",
    "print(\"Ячейка 1: Настройка завершена.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2d5cdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Шаг 2: Чтение и парсинг CSV файла ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка блоков: 100%|██████████| 200/200 [00:00<00:00, 13811.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Парсинг завершен. Всего найдено 2417 отдельных логов.\n",
      "\n",
      "Пример обработанных данных:\n",
      "!!! ВНИМАНИЕ: PII все еще не найдены. Проверьте кодировку или формат данных.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Ячейка 2 (ИСПРАВЛЕННАЯ)\n",
    "\n",
    "def parse_special_csv(file_content):\n",
    "    metadata_pattern = re.compile(r',\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,(.*?),\\s*(\\w+)\\s*$', re.MULTILINE)\n",
    "    header, *rest = file_content.split('\\n', 1)\n",
    "    content = rest[0] if rest else \"\"\n",
    "    last_pos = 0\n",
    "    for match in metadata_pattern.finditer(content):\n",
    "        log_block = content[last_pos:match.start()].strip()\n",
    "        if log_block.startswith('\"') and log_block.endswith('\"'): log_block = log_block[1:-1].replace('\"\"', '\"')\n",
    "        pii_str = match.group(3)\n",
    "        if pii_str.startswith('\"') and pii_str.endswith('\"'): pii_str = pii_str[1:-1].replace('\"\"', '\"')\n",
    "        try:\n",
    "            pii_info = json.loads(pii_str) if pii_str else []\n",
    "        except json.JSONDecodeError:\n",
    "            pii_info = []\n",
    "        yield {'log_block': log_block, 'pii_info': pii_info}\n",
    "        last_pos = match.end()\n",
    "\n",
    "def find_individual_logs(log_block):\n",
    "    return [line for line in log_block.split('\\n') if line.strip()]\n",
    "\n",
    "print(\"--- Шаг 2: Чтение и парсинг CSV файла ---\")\n",
    "with open(input_csv_file, 'r', encoding='utf-8') as infile:\n",
    "    file_content = infile.read()\n",
    "\n",
    "parsed_data = []\n",
    "for block_data in tqdm(list(parse_special_csv(file_content)), desc=\"Обработка блоков\"):\n",
    "    for log_text_raw in find_individual_logs(block_data['log_block']):\n",
    "        \n",
    "        # --- КЛЮЧЕВОЕ ИСПРАВЛЕНИЕ ЗДЕСЬ ---\n",
    "        # Декодируем текст из \"кракозябр\" (CP1251) в нормальный UTF-8\n",
    "        try:\n",
    "            log_text_decoded = log_text_raw.encode('cp1252').decode('cp1251')\n",
    "        except:\n",
    "            log_text_decoded = log_text_raw # Если декодирование не удалось, оставляем как есть\n",
    "        # ------------------------------------\n",
    "\n",
    "        # Теперь ищем PII в ДЕКОДИРОВАННОМ тексте\n",
    "        pii_in_log = [p for p in block_data['pii_info'] if p.get('value') and p['value'] in log_text_decoded]\n",
    "        \n",
    "        # Сохраняем ДЕКОДИРОВАННЫЙ текст для дальнейшей работы\n",
    "        parsed_data.append({'log_text': log_text_decoded, 'pii': pii_in_log})\n",
    "\n",
    "print(f\"\\nПарсинг завершен. Всего найдено {len(parsed_data)} отдельных логов.\")\n",
    "print(\"\\nПример обработанных данных:\")\n",
    "\n",
    "found_pii_example = False\n",
    "for record in parsed_data:\n",
    "    if record['pii']:\n",
    "        print(\"--- НАЙДЕН ПРИМЕР С PII! ---\")\n",
    "        print(record)\n",
    "        found_pii_example = True\n",
    "        break\n",
    "\n",
    "if not found_pii_example:\n",
    "    print(\"!!! ВНИМАНИЕ: PII все еще не найдены. Проверьте кодировку или формат данных.\")\n",
    "else:\n",
    "    print(\"\\nОтлично! PII найдены. Можно переходить к следующей ячейке.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3097b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from evaluate) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from evaluate) (2.3.1)\n",
      "Requirement already satisfied: dill in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from evaluate) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from evaluate) (0.33.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.14)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chulpan\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a56467f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find 'D:/Study/studcamp/НГУ/StudcampNGU\\piiranha_formatted_logs.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# --- ЗАГРУЗКА ---\u001b[39;00m\n\u001b[32m     12\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m full_dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m id2label = {\u001b[38;5;28mint\u001b[39m(k): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m AutoModelForTokenClassification.from_pretrained(model_name).config.id2label.items()}\n\u001b[32m     16\u001b[39m label2id = {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m id2label.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\datasets\\load.py:1392\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1387\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   1388\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   1389\u001b[39m )\n\u001b[32m   1391\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\datasets\\load.py:1132\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1131\u001b[39m     features = _fix_for_backward_compatible_features(features)\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[32m   1142\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\datasets\\load.py:912\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m    889\u001b[39m \u001b[38;5;66;03m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[32m    890\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    891\u001b[39m \u001b[38;5;66;03m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    903\u001b[39m \n\u001b[32m    904\u001b[39m \u001b[38;5;66;03m# Try packaged\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPackagedDatasetModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[38;5;66;03m# Try locally\u001b[39;00m\n\u001b[32m    914\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m path.endswith(filename):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\datasets\\load.py:526\u001b[39m, in \u001b[36mPackagedDatasetModuleFactory.get_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    520\u001b[39m base_path = Path(\u001b[38;5;28mself\u001b[39m.data_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).expanduser().resolve().as_posix()\n\u001b[32m    521\u001b[39m patterns = (\n\u001b[32m    522\u001b[39m     sanitize_patterns(\u001b[38;5;28mself\u001b[39m.data_files)\n\u001b[32m    523\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns(base_path, download_config=\u001b[38;5;28mself\u001b[39m.download_config)\n\u001b[32m    525\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m data_files = \u001b[43mDataFilesDict\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    532\u001b[39m module_path, \u001b[38;5;28mhash\u001b[39m = _PACKAGED_DATASETS_MODULES[\u001b[38;5;28mself\u001b[39m.name]\n\u001b[32m    534\u001b[39m builder_kwargs = {\n\u001b[32m    535\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdata_files\u001b[39m\u001b[33m\"\u001b[39m: data_files,\n\u001b[32m    536\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdataset_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m    537\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\datasets\\data_files.py:689\u001b[39m, in \u001b[36mDataFilesDict.from_patterns\u001b[39m\u001b[34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[39m\n\u001b[32m    684\u001b[39m out = \u001b[38;5;28mcls\u001b[39m()\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns.items():\n\u001b[32m    686\u001b[39m     out[key] = (\n\u001b[32m    687\u001b[39m         patterns_for_key\n\u001b[32m    688\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mDataFilesList\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    695\u001b[39m     )\n\u001b[32m    696\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\datasets\\data_files.py:582\u001b[39m, in \u001b[36mDataFilesList.from_patterns\u001b[39m\u001b[34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[39m\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[32m    580\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    581\u001b[39m         data_files.extend(\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m             \u001b[43mresolve_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m                \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m                \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    588\u001b[39m         )\n\u001b[32m    589\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m    590\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\datasets\\data_files.py:383\u001b[39m, in \u001b[36mresolve_pattern\u001b[39m\u001b[34m(pattern, base_path, allowed_extensions, download_config)\u001b[39m\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    382\u001b[39m         error_msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m with any supported extension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(allowed_extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Unable to find 'D:/Study/studcamp/НГУ/StudcampNGU\\piiranha_formatted_logs.jsonl'"
     ]
    }
   ],
   "source": [
    "# Файл: 1_prepare_data.py\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- КОНФИГУРАЦИЯ ---\n",
    "model_name = \"iiiorg/piiranha-v1-detect-personal-information\"\n",
    "data_file = \"piiranha_formatted_logs.jsonl\"\n",
    "tokenized_dataset_path = \"./tokenized_log_dataset\" # Куда сохранить готовые данные\n",
    "\n",
    "# --- ЗАГРУЗКА ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "full_dataset = load_dataset(\"json\", data_files=data_file, split=\"train\")\n",
    "\n",
    "id2label = {int(k): v for k, v in AutoModelForTokenClassification.from_pretrained(model_name).config.id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# --- ФУНКЦИЯ ТОКЕНИЗАЦИИ ---\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, ner_tags in enumerate(examples[\"ner_labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                tag = ner_tags[word_idx]\n",
    "                label_ids.append(label2id.get(tag, label2id[\"O\"]))\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# --- ОБРАБОТКА И СОХРАНЕНИЕ ---\n",
    "print(\"Начинаю токенизацию данных...\")\n",
    "tokenized_datasets = full_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "print(f\"Сохраняю обработанный датасет в '{tokenized_dataset_path}'...\")\n",
    "tokenized_datasets.save_to_disk(tokenized_dataset_path)\n",
    "\n",
    "print(\"Подготовка данных завершена.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734f4bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Chulpan\\.cache\\huggingface\\hub\\models--iiiorg--piiranha-v1-detect-personal-information. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33miiiorg/piiranha-v1-detect-personal-information\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m model = \u001b[43mAutoModelForTokenClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Подготовка меток на основе вывода модели\u001b[39;00m\n\u001b[32m     18\u001b[39m label_list = model.config.id2label\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:311\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:4680\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4671\u001b[39m     gguf_file\n\u001b[32m   4672\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4673\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4674\u001b[39m ):\n\u001b[32m   4675\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4676\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4677\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4678\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4680\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4682\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4687\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4693\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4695\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4698\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4700\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4701\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:1137\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[39m\n\u001b[32m   1122\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1123\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m   1124\u001b[39m     cached_file_kwargs = {\n\u001b[32m   1125\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcache_dir\u001b[39m\u001b[33m\"\u001b[39m: cache_dir,\n\u001b[32m   1126\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mforce_download\u001b[39m\u001b[33m\"\u001b[39m: force_download,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1135\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m: commit_hash,\n\u001b[32m   1136\u001b[39m     }\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m     resolved_archive_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[32m   1140\u001b[39m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[32m   1141\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[32m   1142\u001b[39m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:312\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    255\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    256\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    257\u001b[39m     **kwargs,\n\u001b[32m    258\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    259\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    261\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    310\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:470\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    469\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    485\u001b[39m         snapshot_download(\n\u001b[32m    486\u001b[39m             path_or_repo_id,\n\u001b[32m    487\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    496\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    497\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1008\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    989\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    990\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1005\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1006\u001b[39m     )\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1161\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1158\u001b[39m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[32m   1160\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1173\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1174\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1725\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1718\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1719\u001b[39m             logger.warning(\n\u001b[32m   1720\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo, but the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhf_xet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m package is not installed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1721\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFalling back to regular HTTP download. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1722\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1723\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1725\u001b[39m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1726\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1727\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1728\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1729\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1730\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1731\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1732\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1734\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1735\u001b[39m _chmod_and_move(incomplete_path, destination_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:494\u001b[39m, in \u001b[36mhttp_get\u001b[39m\u001b[34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[39m\n\u001b[32m    492\u001b[39m new_resume_size = resume_size\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[32m    496\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\urllib3\\response.py:1066\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1069\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\urllib3\\response.py:955\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    952\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    953\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    957\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\urllib3\\response.py:879\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    876\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    882\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    887\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    888\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    889\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\site-packages\\urllib3\\response.py:862\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    860\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    861\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\http\\client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chulpan\\miniconda3\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Файл: 2_train_model.py\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# --- КОНФИГУРАЦИЯ ---\n",
    "model_name = \"iiiorg/piiranha-v1-detect-personal-information\"\n",
    "tokenized_dataset_path = \"./tokenized_log_dataset\"\n",
    "output_dir = \"piiranha-finetuned-logs\"\n",
    "\n",
    "# --- ЗАГРУЗКА ---\n",
    "print(\"Загружаю подготовленный датасет...\")\n",
    "tokenized_datasets_from_disk = load_from_disk(tokenized_dataset_path)\n",
    "dataset_dict = tokenized_datasets_from_disk.train_test_split(test_size=0.1)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# --- МЕТРИКИ ---\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "label_list = list(model.config.id2label.values())\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# --- НАСТРОЙКИ ОБУЧЕНИЯ ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# --- ОБУЧЕНИЕ ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Начинаю дообучение модели...\")\n",
    "trainer.train()\n",
    "\n",
    "# --- СОХРАНЕНИЕ ---\n",
    "final_model_path = f\"{output_dir}-final\"\n",
    "trainer.save_model(final_model_path)\n",
    "print(f\"\\nОбучение завершено. Модель сохранена в '{final_model_path}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
