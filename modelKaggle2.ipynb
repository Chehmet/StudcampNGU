{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12540295,"sourceType":"datasetVersion","datasetId":7916908},{"sourceId":12543667,"sourceType":"datasetVersion","datasetId":7919379},{"sourceId":12544616,"sourceType":"datasetVersion","datasetId":7920084}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom tqdm.auto import tqdm ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T12:55:47.196325Z","iopub.execute_input":"2025-07-22T12:55:47.197480Z","iopub.status.idle":"2025-07-22T12:55:47.201855Z","shell.execute_reply.started":"2025-07-22T12:55:47.197437Z","shell.execute_reply":"2025-07-22T12:55:47.201189Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import ast\nfrom datasets import load_dataset, concatenate_datasets\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom tqdm.auto import tqdm\nimport random\n\n# --- КОНФИГУРАЦИЯ ---\nmodel_name = \"iiiorg/piiranha-v1-detect-personal-information\"\ndata_file = \"/kaggle/input/dataset3/more_pi_dataset.csv\" \ntokenized_dataset_path = \"./tokenized_log_dataset\"\n\n# --- ЗАГРУЗКА ---\nprint(\"--- Шаг 1: Загрузка токенизатора и датасета ---\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nfull_dataset = load_dataset(\"csv\", data_files=data_file, split=\"train\")\nprint(\"Датасет загружен. Всего записей:\", len(full_dataset))\n\n\n# --- ФУНКЦИЯ ПРЕОБРАЗОВАНИЯ ---\ndef prepare_dataset(examples):\n    examples[\"tokens\"] = [ast.literal_eval(tok_list) for tok_list in examples[\"mbert_tokens\"]]\n    examples[\"ner_labels\"] = [ast.literal_eval(label_list) for label_list in examples[\"mbert_token_classes\"]]\n    return examples\n\nprint(\"\\n--- Шаг 2: Преобразование строковых колонок в списки ---\")\ncolumns_to_remove = [col for col in full_dataset.column_names if col not in ['mbert_tokens', 'mbert_token_classes']]\nprepared_dataset = full_dataset.map(\n    prepare_dataset,\n    batched=True,\n    num_proc=2,\n    remove_columns=columns_to_remove,\n    desc=\"Парсинг токенов и меток\"\n)\nprint(\"Преобразование завершено.\")\n\n\n# --- ИЗМЕНЕНИЕ: НОВЫЙ ШАГ БАЛАНСИРОВКИ ---\nprint(\"\\n--- Шаг 2.5: Балансировка датасета (Undersampling) ---\")\n\n# Функция для проверки, есть ли в примере хоть одна PII метка\ndef has_pii(example):\n    # set(example['ner_labels']) - {'O'} вернет True, если есть что-то кроме 'O'\n    return bool(set(example['ner_labels']) - {'O'})\n\n# Разделяем датасет на две части\npii_dataset = prepared_dataset.filter(has_pii, num_proc=2)\nno_pii_dataset = prepared_dataset.filter(lambda x: not has_pii(x), num_proc=2)\n\nprint(f\"Найдено {len(pii_dataset)} записей с PII.\")\nprint(f\"Найдено {len(no_pii_dataset)} записей без PII (только 'O').\")\n\n# Определяем, сколько \"пустых\" записей мы хотим оставить.\n# Сделаем их количество равным количеству записей с PII (соотношение 1:1)\nnum_pii_samples = len(pii_dataset)\n\n# Перемешиваем \"пустые\" записи и выбираем нужное количество\n# seed=42 делает выбор случайным, но воспроизводимым\nif len(no_pii_dataset) > num_pii_samples:\n    print(f\"Уменьшаю количество записей без PII с {len(no_pii_dataset)} до {num_pii_samples}.\")\n    no_pii_dataset_balanced = no_pii_dataset.shuffle(seed=42).select(range(num_pii_samples))\nelse:\n    print(\"Количество записей без PII уже меньше или равно количеству записей с PII. Undersampling не требуется.\")\n    no_pii_dataset_balanced = no_pii_dataset\n\n# Объединяем два датасета обратно в один, теперь сбалансированный\nbalanced_dataset = concatenate_datasets([pii_dataset, no_pii_dataset_balanced])\n# Перемешиваем финальный датасет, чтобы PII и не-PII примеры шли вперемешку\nbalanced_dataset = balanced_dataset.shuffle(seed=42)\n\nprint(f\"Размер нового сбалансированного датасета: {len(balanced_dataset)} записей.\")\n\n\n# --- Получение меток из модели ---\nmodel_config = AutoModelForTokenClassification.from_pretrained(model_name).config\nid2label = {int(k): v for k, v in model_config.id2label.items()}\nlabel2id = {v: k for k, v in id2label.items()}\nprint(\"\\nСловарь меток (label2id) из модели:\", label2id)\n\n\n# --- ФУНКЦИЯ ТОКЕНИЗАЦИИ (остается без изменений) ---\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"],\n        truncation=True,\n        is_split_into_words=True,\n        max_length=512,\n    )\n    all_labels = []\n    for i, ner_tags in enumerate(examples[\"ner_labels\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                tag = ner_tags[word_idx]\n                label_ids.append(label2id.get(tag, label2id[\"O\"]))\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        all_labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = all_labels\n    return tokenized_inputs\n\n# --- ОБРАБОТКА И СОХРАНЕНИЕ ---\nprint(\"\\n--- Шаг 3: Начинаю токенизацию сбалансированных данных ---\")\n# Применяем токенизацию к нашему новому `balanced_dataset`\ntokenized_datasets = balanced_dataset.map(\n    tokenize_and_align_labels,\n    batched=True,\n    num_proc=2,\n    remove_columns=balanced_dataset.column_names,\n    desc=\"Токенизация логов\"\n)\nprint(\"Токенизация завершена.\")\n\n\nprint(f\"\\n--- Шаг 4: Сохраняю обработанный датасет в '{tokenized_dataset_path}' ---\")\ntokenized_datasets.save_to_disk(tokenized_dataset_path)\nprint(\"Подготовка данных успешно завершена!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T12:55:47.203278Z","iopub.execute_input":"2025-07-22T12:55:47.203552Z","iopub.status.idle":"2025-07-22T12:55:49.964011Z","shell.execute_reply.started":"2025-07-22T12:55:47.203534Z","shell.execute_reply":"2025-07-22T12:55:49.963006Z"}},"outputs":[{"name":"stdout","text":"--- Шаг 1: Загрузка токенизатора и датасета ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"442e9890b43c45edb3fc57a447e03a42"}},"metadata":{}},{"name":"stdout","text":"Датасет загружен. Всего записей: 100\n\n--- Шаг 2: Преобразование строковых колонок в списки ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Парсинг токенов и меток (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48a1a6153ce94e85b492cb636e0cf79c"}},"metadata":{}},{"name":"stdout","text":"Преобразование завершено.\n\n--- Шаг 2.5: Балансировка датасета (Undersampling) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ed8764e8646421ba8072db8da7270ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87770009af4f4acaaf715a526957b22f"}},"metadata":{}},{"name":"stdout","text":"Найдено 100 записей с PII.\nНайдено 0 записей без PII (только 'O').\nКоличество записей без PII уже меньше или равно количеству записей с PII. Undersampling не требуется.\nРазмер нового сбалансированного датасета: 100 записей.\n\nСловарь меток (label2id) из модели: {'I-ACCOUNTNUM': 0, 'I-BUILDINGNUM': 1, 'I-CITY': 2, 'I-CREDITCARDNUMBER': 3, 'I-DATEOFBIRTH': 4, 'I-DRIVERLICENSENUM': 5, 'I-EMAIL': 6, 'I-GIVENNAME': 7, 'I-IDCARDNUM': 8, 'I-PASSWORD': 9, 'I-SOCIALNUM': 10, 'I-STREET': 11, 'I-SURNAME': 12, 'I-TAXNUM': 13, 'I-TELEPHONENUM': 14, 'I-USERNAME': 15, 'I-ZIPCODE': 16, 'O': 17}\n\n--- Шаг 3: Начинаю токенизацию сбалансированных данных ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Токенизация логов (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d6c525fc8f0496190d38f2873471d71"}},"metadata":{}},{"name":"stdout","text":"Токенизация завершена.\n\n--- Шаг 4: Сохраняю обработанный датасет в './tokenized_log_dataset' ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"318af61c23db479c96bdd0dd25b05924"}},"metadata":{}},{"name":"stdout","text":"Подготовка данных успешно завершена!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!pip install seqeval\n!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T12:55:49.965111Z","iopub.execute_input":"2025-07-22T12:55:49.965358Z","iopub.status.idle":"2025-07-22T12:55:56.949677Z","shell.execute_reply.started":"2025-07-22T12:55:49.965333Z","shell.execute_reply":"2025-07-22T12:55:56.948892Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# import transformers\n# print(transformers.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T12:55:56.950835Z","iopub.execute_input":"2025-07-22T12:55:56.951088Z","iopub.status.idle":"2025-07-22T12:55:56.955228Z","shell.execute_reply.started":"2025-07-22T12:55:56.951048Z","shell.execute_reply":"2025-07-22T12:55:56.954500Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# !pip install --upgrade --force-reinstall \"numpy<2\" \"datasets\" \"transformers\" \"torch\" \"evaluate\" \"seqeval\" \"scikit-learn\" \"tqdm\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T12:55:56.956898Z","iopub.execute_input":"2025-07-22T12:55:56.957147Z","iopub.status.idle":"2025-07-22T12:55:56.977848Z","shell.execute_reply.started":"2025-07-22T12:55:56.957126Z","shell.execute_reply":"2025-07-22T12:55:56.977100Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from datasets import load_from_disk\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForTokenClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForTokenClassification,\n    pipeline  # <-- ИЗМЕНЕНИЕ 1: Импортируем pipeline для удобного предсказания\n)\nimport evaluate\nimport numpy as np\nimport pandas as pd # <-- ИЗМЕНЕНИЕ 2: Импортируем pandas для красивого вывода\nfrom tqdm.auto import tqdm\n\n# --- КОНФИГУРАЦИЯ ---\nmodel_name = \"iiiorg/piiranha-v1-detect-personal-information\"\ntokenized_dataset_path = \"./tokenized_log_dataset\"\noutput_dir = \"piiranha-finetuned-logs\"\nfinal_model_path = f\"{output_dir}-final\"\n\n# --- ЗАГРУЗКА ---\nprint(\"Загружаю подготовленный датасет...\")\ntokenized_datasets_from_disk = load_from_disk(tokenized_dataset_path)\ndataset_dict = tokenized_datasets_from_disk.train_test_split(test_size=0.1, seed=42) # Добавим seed для воспроизводимости\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForTokenClassification.from_pretrained(model_name)\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n\n# --- МЕТРИКИ (ОБНОВЛЕННАЯ ВЕРСИЯ) ---\nseqeval = evaluate.load(\"seqeval\")\nlabel_list = list(model.config.id2label.values())\n\n# ИЗМЕНЕНИЕ 3: Расширяем функцию метрик для детального отчета\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    # Получаем полный отчет от seqeval\n    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n    \n    # Собираем метрики по каждому классу PII\n    per_class_results = {}\n    for key, value in results.items():\n        if isinstance(value, dict) and 'f1-score' in value:\n             per_class_results[f\"{key}_precision\"] = value['precision']\n             per_class_results[f\"{key}_recall\"] = value['recall']\n             per_class_results[f\"{key}_f1\"] = value['f1-score']\n\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n        **per_class_results # Добавляем метрики по классам\n    }\n\n# --- НАСТРОЙКИ ОБУЧЕНИЯ ---\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3, # Возможно, стоит увеличить до 5-10, раз модель не учится\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    logging_steps=10,\n    report_to=\"none\",\n)\n\n# --- ОБУЧЕНИЕ ---\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset_dict[\"train\"],\n    eval_dataset=dataset_dict[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\nprint(\"Начинаю дообучение модели...\")\ntrainer.train()\n\n# --- СОХРАНЕНИЕ ---\ntrainer.save_model(final_model_path)\nprint(f\"\\nОбучение завершено. Модель сохранена в '{final_model_path}'\")\n\n# --- ИЗМЕНЕНИЕ 4: ФУНКЦИЯ ДЛЯ ПРОСМОТРА ПРЕДСКАЗАНИЙ ---\ndef show_predictions(model_path, num_examples=3):\n    \"\"\"Загружает модель и показывает ее предсказания на нескольких примерах из тестового сета.\"\"\"\n    print(\"\\n--- Анализ предсказаний модели ---\")\n    \n    # Загружаем лучшую сохраненную модель\n    ner_pipe = pipeline(\"token-classification\", model=model_path, aggregation_strategy=\"simple\")\n    \n    test_set = dataset_dict[\"test\"]\n    \n    for i in range(num_examples):\n        if i >= len(test_set):\n            break\n        \n        example = test_set[i]\n        tokens = tokenizer.convert_ids_to_tokens(example['input_ids'])\n        true_labels = [label_list[l] if l != -100 else \"PAD\" for l in example['labels']]\n        \n        # Склеиваем токены обратно в текст для pipeline\n        text = tokenizer.decode(example['input_ids'], skip_special_tokens=True)\n        \n        print(f\"\\n--- Пример #{i+1} ---\")\n        print(f\"Текст: {text}\")\n        \n        predictions = ner_pipe(text)\n        \n        # Создаем словарь предсказаний для удобства\n        pred_map = {}\n        for pred in predictions:\n            # Pipeline может склеивать B- и I- токены. Нам нужно найти начальный токен.\n            start_token_index = tokenizer(pred['word'], add_special_tokens=False).input_ids[0]\n            for tok_idx in tokenizer(pred['word'], add_special_tokens=False).input_ids:\n                 pred_map[tok_idx] = pred['entity_group']\n\n        # Создаем DataFrame для наглядного сравнения\n        results_df = pd.DataFrame({\n            \"Токен\": tokens,\n            \"Настоящая метка\": true_labels\n        })\n        \n        # Добавляем предсказания, если они есть\n        predicted_labels = []\n        for j, token_id in enumerate(example['input_ids']):\n            if true_labels[j] == \"PAD\": # Пропускаем паддинг\n                continue\n            \n            # Находим предсказание для этого токена\n            # ВАЖНО: этот метод не идеален, т.к. pipeline агрегирует токены,\n            # но для визуальной проверки он хорошо подходит.\n            # Более точный способ требует ручного прогона модели и сопоставления.\n            # Сейчас для простоты оставим так.\n            \n            # Более простой и надежный способ: прогоним токенизированный ввод через модель\n        \n    # Более точный способ без pipeline\n    print(\"\\n--- Точный анализ предсказаний (без pipeline) ---\")\n    model.eval()\n    for i in range(num_examples):\n        example = test_set[i]\n        input_ids = torch.tensor(example['input_ids']).unsqueeze(0).to(model.device)\n        \n        with torch.no_grad():\n            logits = model(input_ids).logits\n        \n        predictions = torch.argmax(logits, dim=2).squeeze().tolist()\n        \n        tokens = tokenizer.convert_ids_to_tokens(example['input_ids'])\n        true_labels = [label_list[l] if l != -100 else \"PAD\" for l in example['labels']]\n        pred_labels = [label_list[p] for p in predictions]\n        \n        df = pd.DataFrame({\n            'Токен': tokens, \n            'Реальная метка': true_labels, \n            'Предсказание модели': pred_labels\n        })\n        \n        # Фильтруем PAD и спец.токены для чистоты вывода\n        df_filtered = df[(df['Реальная метка'] != 'PAD') & (~df['Токен'].isin(['[CLS]', '[SEP]', '[PAD]']))]\n        \n        print(f\"\\n--- Пример #{i+1} ---\")\n        \n        # Выводим только строки, где есть расхождения или где есть PII\n        mismatches = df_filtered[df_filtered['Реальная метка'] != df_filtered['Предсказание модели']]\n        real_pii = df_filtered[df_filtered['Реальная метка'] != 'O']\n        \n        display_df = pd.concat([real_pii, mismatches]).drop_duplicates().sort_index()\n\n        if display_df.empty:\n            print(\"Расхождений не найдено, и в примере нет PII.\")\n        else:\n            print(display_df.to_string())\n\n# Запускаем нашу новую функцию\nimport torch # Нужно для точного анализа\nshow_predictions(final_model_path, num_examples=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T12:55:56.978774Z","iopub.execute_input":"2025-07-22T12:55:56.979022Z","iopub.status.idle":"2025-07-22T12:56:45.730071Z","shell.execute_reply.started":"2025-07-22T12:55:56.979000Z","shell.execute_reply":"2025-07-22T12:56:45.728900Z"}},"outputs":[{"name":"stdout","text":"Загружаю подготовленный датасет...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/1118594090.py:82: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Начинаю дообучение модели...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='19' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [18/18 00:30, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.356286</td>\n      <td>0.040000</td>\n      <td>0.047619</td>\n      <td>0.043478</td>\n      <td>0.922942</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.414200</td>\n      <td>0.181793</td>\n      <td>0.068966</td>\n      <td>0.095238</td>\n      <td>0.080000</td>\n      <td>0.940455</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.414200</td>\n      <td>0.177600</td>\n      <td>0.080000</td>\n      <td>0.095238</td>\n      <td>0.086957</td>\n      <td>0.937828</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/2: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/1118594090.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Начинаю дообучение модели...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# --- СОХРАНЕНИЕ ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2656\u001b[0;31m             self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2657\u001b[0m                 \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3102\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial)\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_only_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3209\u001b[0m             \u001b[0;31m# Save optimizer and scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3210\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_optimizer_and_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3211\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m             \u001b[0;31m# Save RNG state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   3335\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3336\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3337\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3339\u001b[0m         \u001b[0;31m# Save SCHEDULER & SCALER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             _save(\n\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:626] . unexpected pos 1129750976 vs 1129750864"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:626] . unexpected pos 1129750976 vs 1129750864","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T12:56:45.730613Z","iopub.status.idle":"2025-07-22T12:56:45.730903Z","shell.execute_reply.started":"2025-07-22T12:56:45.730751Z","shell.execute_reply":"2025-07-22T12:56:45.730780Z"}},"outputs":[],"execution_count":null}]}