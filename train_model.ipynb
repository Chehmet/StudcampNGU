{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T05:07:22.576820Z",
     "iopub.status.busy": "2025-07-23T05:07:22.575803Z",
     "iopub.status.idle": "2025-07-23T05:07:22.633563Z",
     "shell.execute_reply": "2025-07-23T05:07:22.632349Z",
     "shell.execute_reply.started": "2025-07-23T05:07:22.576763Z"
    }
   },
   "source": [
    "**Проверка версий библиотек и очистка кеша моделей. Установка библиотек**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-23T05:15:49.953137Z",
     "iopub.status.busy": "2025-07-23T05:15:49.951537Z",
     "iopub.status.idle": "2025-07-23T05:16:12.828951Z",
     "shell.execute_reply": "2025-07-23T05:16:12.827597Z",
     "shell.execute_reply.started": "2025-07-23T05:15:49.953085Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seqeval in /home/jupyter/.local/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: evaluate in /home/jupyter/.local/lib/python3.10/site-packages (0.4.5)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.33.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.7)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.12.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->datasets>=2.0.0->evaluate) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets>=2.0.0->evaluate) (3.10)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.7.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "!rm -rf ~/.cache/huggingface/hub/models--iiiorg--piiranha-v1-detect-personal-information\n",
    "%pip install seqeval\n",
    "%pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запуск токенизации и выравнивание датасета. Приведение к формату для обучения**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T05:16:12.834296Z",
     "iopub.status.busy": "2025-07-23T05:16:12.831564Z",
     "iopub.status.idle": "2025-07-23T05:18:06.054331Z",
     "shell.execute_reply": "2025-07-23T05:18:06.052974Z",
     "shell.execute_reply.started": "2025-07-23T05:16:12.834196Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Шаг 1: Загрузка токенизатора и датасета ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасет загружен. Всего записей: 100000\n",
      "\n",
      "--- Шаг 2: Преобразование строковых колонок в списки ---\n",
      "Преобразование завершено.\n",
      "\n",
      "--- Шаг 2.5: Балансировка датасета (Undersampling) ---\n",
      "Найдено 100000 записей с PII.\n",
      "Найдено 0 записей без PII (только 'O').\n",
      "Количество записей без PII уже меньше или равно количеству записей с PII. Undersampling не требуется.\n",
      "Размер нового сбалансированного датасета: 100000 записей.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Словарь меток (label2id) из модели: {'I-ACCOUNTNUM': 0, 'I-BUILDINGNUM': 1, 'I-CITY': 2, 'I-CREDITCARDNUMBER': 3, 'I-DATEOFBIRTH': 4, 'I-DRIVERLICENSENUM': 5, 'I-EMAIL': 6, 'I-GIVENNAME': 7, 'I-IDCARDNUM': 8, 'I-PASSWORD': 9, 'I-SOCIALNUM': 10, 'I-STREET': 11, 'I-SURNAME': 12, 'I-TAXNUM': 13, 'I-TELEPHONENUM': 14, 'I-USERNAME': 15, 'I-ZIPCODE': 16, 'O': 17}\n",
      "\n",
      "--- Шаг 3: Начинаю токенизацию сбалансированных данных ---\n",
      "Токенизация завершена.\n",
      "\n",
      "--- Шаг 4: Сохраняю обработанный датасет в './tokenized_log_dataset' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 100000/100000 [00:20<00:00, 4989.40 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подготовка данных успешно завершена!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "# --- КОНФИГУРАЦИЯ ---\n",
    "model_name = \"iiiorg/piiranha-v1-detect-personal-information\"\n",
    "data_file = \"more_pi_dataset (2).csv\" \n",
    "tokenized_dataset_path = \"./tokenized_log_dataset\"\n",
    "\n",
    "# --- ЗАГРУЗКА ---\n",
    "print(\"--- Шаг 1: Загрузка токенизатора и датасета ---\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "full_dataset = load_dataset(\"csv\", data_files=data_file, split=\"train\")\n",
    "print(\"Датасет загружен. Всего записей:\", len(full_dataset))\n",
    "\n",
    "\n",
    "# --- ФУНКЦИЯ ПРЕОБРАЗОВАНИЯ ---\n",
    "def prepare_dataset(examples):\n",
    "    examples[\"tokens\"] = [ast.literal_eval(tok_list) for tok_list in examples[\"mbert_tokens\"]]\n",
    "    examples[\"ner_labels\"] = [ast.literal_eval(label_list) for label_list in examples[\"mbert_token_classes\"]]\n",
    "    return examples\n",
    "\n",
    "print(\"\\n--- Шаг 2: Преобразование строковых колонок в списки ---\")\n",
    "columns_to_remove = [col for col in full_dataset.column_names if col not in ['mbert_tokens', 'mbert_token_classes']]\n",
    "prepared_dataset = full_dataset.map(\n",
    "    prepare_dataset,\n",
    "    batched=True,\n",
    "    num_proc=2,\n",
    "    remove_columns=columns_to_remove,\n",
    "    desc=\"Парсинг токенов и меток\"\n",
    ")\n",
    "print(\"Преобразование завершено.\")\n",
    "\n",
    "\n",
    "# --- ИЗМЕНЕНИЕ: НОВЫЙ ШАГ БАЛАНСИРОВКИ ---\n",
    "print(\"\\n--- Шаг 2.5: Балансировка датасета (Undersampling) ---\")\n",
    "\n",
    "# Функция для проверки, есть ли в примере хоть одна PII метка\n",
    "def has_pii(example):\n",
    "    # set(example['ner_labels']) - {'O'} вернет True, если есть что-то кроме 'O'\n",
    "    return bool(set(example['ner_labels']) - {'O'})\n",
    "\n",
    "# Разделяем датасет на две части\n",
    "pii_dataset = prepared_dataset.filter(has_pii, num_proc=2)\n",
    "no_pii_dataset = prepared_dataset.filter(lambda x: not has_pii(x), num_proc=2)\n",
    "\n",
    "print(f\"Найдено {len(pii_dataset)} записей с PII.\")\n",
    "print(f\"Найдено {len(no_pii_dataset)} записей без PII (только 'O').\")\n",
    "\n",
    "# Определяем, сколько \"пустых\" записей мы хотим оставить.\n",
    "# Сделаем их количество равным количеству записей с PII (соотношение 1:1)\n",
    "num_pii_samples = len(pii_dataset)\n",
    "\n",
    "# Перемешиваем \"пустые\" записи и выбираем нужное количество\n",
    "# seed=42 делает выбор случайным, но воспроизводимым\n",
    "if len(no_pii_dataset) > num_pii_samples:\n",
    "    print(f\"Уменьшаю количество записей без PII с {len(no_pii_dataset)} до {num_pii_samples}.\")\n",
    "    no_pii_dataset_balanced = no_pii_dataset.shuffle(seed=42).select(range(num_pii_samples))\n",
    "else:\n",
    "    print(\"Количество записей без PII уже меньше или равно количеству записей с PII. Undersampling не требуется.\")\n",
    "    no_pii_dataset_balanced = no_pii_dataset\n",
    "\n",
    "# Объединяем два датасета обратно в один, теперь сбалансированный\n",
    "balanced_dataset = concatenate_datasets([pii_dataset, no_pii_dataset_balanced])\n",
    "# Перемешиваем финальный датасет, чтобы PII и не-PII примеры шли вперемешку\n",
    "balanced_dataset = balanced_dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"Размер нового сбалансированного датасета: {len(balanced_dataset)} записей.\")\n",
    "\n",
    "\n",
    "# --- Получение меток из модели ---\n",
    "model_config = AutoModelForTokenClassification.from_pretrained(model_name).config\n",
    "id2label = {int(k): v for k, v in model_config.id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "print(\"\\nСловарь меток (label2id) из модели:\", label2id)\n",
    "\n",
    "\n",
    "# --- ФУНКЦИЯ ТОКЕНИЗАЦИИ ДЛЯ DEBERTA (без использования word_ids) ---\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # Токенизируем входные токены (уже списки слов)\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        add_special_tokens=True  # Добавляем [CLS] и [SEP] автоматически\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "\n",
    "    for i, label_list in enumerate(examples[\"ner_labels\"]):\n",
    "        # Получаем ID токенов\n",
    "        input_ids = tokenized_inputs[\"input_ids\"][i]\n",
    "        \n",
    "        # Список для выровненных меток\n",
    "        aligned_labels = []\n",
    "        \n",
    "        # Индекс в оригинальных словах\n",
    "        word_idx = 0\n",
    "        \n",
    "        # Пропускаем первый токен ([CLS])\n",
    "        aligned_labels.append(-100)\n",
    "        \n",
    "        # Обрабатываем каждый токен\n",
    "        for j, token_id in enumerate(input_ids[1:], 1):  # Начинаем с 1, пропуская [CLS]\n",
    "            token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "            \n",
    "            # Проверяем, является ли это [SEP]\n",
    "            if token_id == tokenizer.sep_token_id:\n",
    "                aligned_labels.append(-100)\n",
    "                break  # Завершаем обработку для этой последовательности\n",
    "            \n",
    "            # Если токен — начало нового слова (не _...)\n",
    "            if not token.startswith(\"▁\"):  # DeBERTa использует ▁ для начала слов\n",
    "                # Это подслово — пропускаем (-100)\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                # Это начало нового слова\n",
    "                if word_idx < len(label_list):\n",
    "                    tag = label_list[word_idx]\n",
    "                    aligned_labels.append(label2id.get(tag, label2id[\"O\"]))\n",
    "                    word_idx += 1\n",
    "                else:\n",
    "                    aligned_labels.append(label2id[\"O\"])\n",
    "                    word_idx += 1\n",
    "        \n",
    "        # Добавляем метку для [SEP] если он еще не добавлен\n",
    "        if len(aligned_labels) < len(input_ids):\n",
    "            aligned_labels.append(-100)\n",
    "        \n",
    "        # Обрезаем или дополняем до нужной длины\n",
    "        if len(aligned_labels) > 512:\n",
    "            aligned_labels = aligned_labels[:512]\n",
    "        elif len(aligned_labels) < len(input_ids):\n",
    "            aligned_labels += [-100] * (len(input_ids) - len(aligned_labels))\n",
    "        \n",
    "        all_labels.append(aligned_labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# --- ОБРАБОТКА И СОХРАНЕНИЕ ---\n",
    "print(\"\\n--- Шаг 3: Начинаю токенизацию сбалансированных данных ---\")\n",
    "# Применяем токенизацию к нашему новому `balanced_dataset`\n",
    "tokenized_datasets = balanced_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    num_proc=2,\n",
    "    remove_columns=balanced_dataset.column_names,\n",
    "    desc=\"Токенизация логов\"\n",
    ")\n",
    "print(\"Токенизация завершена.\")\n",
    "\n",
    "\n",
    "print(f\"\\n--- Шаг 4: Сохраняю обработанный датасет в '{tokenized_dataset_path}' ---\")\n",
    "tokenized_datasets.save_to_disk(tokenized_dataset_path)\n",
    "print(\"Подготовка данных успешно завершена!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дообучение модели(BaseLine)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T05:18:06.058545Z",
     "iopub.status.busy": "2025-07-23T05:18:06.056201Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаю подготовленный датасет...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    pipeline\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "\n",
    "# --- ИЗМЕНЕНИЕ 1: Добавляем импорты для графиков ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- КОНФИГУРАЦИЯ ---\n",
    "model_name = \"iiiorg/piiranha-v1-detect-personal-information\"\n",
    "tokenized_dataset_path = \"./tokenized_log_dataset\"\n",
    "output_dir = \"piiranha-finetuned-logs\"\n",
    "final_model_path = f\"{output_dir}-final\"\n",
    "\n",
    "# --- ЗАГРУЗКА ---\n",
    "print(\"Загружаю подготовленный датасет...\")\n",
    "tokenized_datasets_from_disk = load_from_disk(tokenized_dataset_path)\n",
    "dataset_dict = tokenized_datasets_from_disk.train_test_split(test_size=0.2, seed=42) # Увеличим тест. выборку для более стабильной оценки\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# --- МЕТРИКИ ---\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "label_list = list(model.config.id2label.values())\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "    per_class_results = {}\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, dict) and 'f1-score' in value:\n",
    "             per_class_results[f\"eval_{key}_f1\"] = value['f1-score']\n",
    "    \n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "        **per_class_results\n",
    "    }\n",
    "\n",
    "# --- ИЗМЕНЕНИЕ 2: Оптимизируем настройки обучения ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=3e-5,  # Немного увеличим для ускорения сходимости\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15, # Оставляем 15 эпох\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Стратегии логирования, оценки и сохранения\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",  # ИСПРАВЛЕНО: было eval_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "    # Ключевые параметры для управления памятью и выбором модели\n",
    "    load_best_model_at_end=True,      # Загрузить лучшую модель в конце обучения\n",
    "    metric_for_best_model=\"f1\",       # Считать лучшей по общей F1-мере\n",
    "    greater_is_better=True,           # Чем больше F1, тем лучше\n",
    "    save_total_limit=1,               # Хранить ТОЛЬКО 1 лучший чекпоинт\n",
    "    \n",
    "    report_to=\"none\", # Отключаем интеграции вроде wandb\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,  # Остановить, если f1 не улучшается 3 эпохи подряд\n",
    "    early_stopping_threshold=0.001, # Улучшением считается рост f1 хотя бы на 0.001\n",
    ")\n",
    "\n",
    "# --- ОБУЧЕНИЕ ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")\n",
    "\n",
    "print(\"Начинаю дообучение модели...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# --- СОХРАНЕНИЕ ЛУЧШЕЙ МОДЕЛИ ---\n",
    "trainer.save_model(final_model_path)\n",
    "print(f\"\\nОбучение завершено. Лучшая модель сохранена в '{final_model_path}'\")\n",
    "print(\"Информация о тренировке:\", train_result)\n",
    "\n",
    "# --- ФУНКЦИЯ ДЛЯ ПРОСМОТРА ПРЕДСКАЗАНИЙ (остается без изменений) ---\n",
    "def show_predictions(model_path, num_examples=5):\n",
    "    print(\"\\n--- Точный анализ предсказаний (без pipeline) ---\")\n",
    "    # Загружаем лучшую модель для анализа\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    test_set = dataset_dict[\"test\"]\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        if i >= len(test_set): break\n",
    "        \n",
    "        example = test_set[i]\n",
    "        input_ids = torch.tensor(example['input_ids']).unsqueeze(0).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids).logits\n",
    "        \n",
    "        predictions = torch.argmax(logits, dim=2).squeeze().tolist()\n",
    "        \n",
    "        tokens = tokenizer.convert_ids_to_tokens(example['input_ids'])\n",
    "        true_labels = [label_list[l] if l != -100 else \"PAD\" for l in example['labels']]\n",
    "        pred_labels = [label_list[p] for p in predictions]\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'Токен': tokens, \n",
    "            'Реальная метка': true_labels, \n",
    "            'Предсказание модели': pred_labels\n",
    "        })\n",
    "        \n",
    "        df_filtered = df[(df['Реальная метка'] != 'PAD') & (~df['Токен'].isin(['[CLS]', '[SEP]', '[PAD]']))]\n",
    "        \n",
    "        print(f\"\\n--- Пример #{i+1} ---\")\n",
    "        \n",
    "        mismatches = df_filtered[df_filtered['Реальная метка'] != df_filtered['Предсказание модели']]\n",
    "        real_pii = df_filtered[df_filtered['Реальная метка'] != 'O']\n",
    "        \n",
    "        display_df = pd.concat([real_pii, mismatches]).drop_duplicates().sort_index()\n",
    "\n",
    "        if display_df.empty:\n",
    "            print(\"Расхождений не найдено, и в примере нет PII.\")\n",
    "        else:\n",
    "            print(display_df.to_string())\n",
    "\n",
    "# Запускаем анализ предсказаний на лучшей сохраненной модели\n",
    "show_predictions(final_model_path, num_examples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метрики**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_history(log_history):\n",
    "    print(\"\\n--- Построение графиков обучения ---\")\n",
    "    \n",
    "    # Преобразуем историю в DataFrame\n",
    "    logs = [log for log in log_history if 'loss' in log or 'eval_loss' in log]\n",
    "    df = pd.DataFrame(logs)\n",
    "\n",
    "    # Разделяем на train и eval\n",
    "    train_df = df[df['loss'].notna()].reset_index(drop=True)\n",
    "    eval_df = df[df['eval_loss'].notna()].reset_index(drop=True)\n",
    "\n",
    "    # Настраиваем стиль графиков\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    fig.suptitle('История обучения модели', fontsize=16)\n",
    "\n",
    "    # 1. График потерь (Loss)\n",
    "    axs[0].plot(train_df['epoch'], train_df['loss'], label='Train Loss', marker='o')\n",
    "    axs[0].plot(eval_df['epoch'], eval_df['eval_loss'], label='Validation Loss', marker='o')\n",
    "    axs[0].set_title('Потери на обучении и валидации')\n",
    "    axs[0].set_xlabel('Эпоха')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # 2. График основных метрик (Precision, Recall, F1)\n",
    "    axs[1].plot(eval_df['epoch'], eval_df['eval_precision'], label='Precision', marker='o')\n",
    "    axs[1].plot(eval_df['epoch'], eval_df['eval_recall'], label='Recall', marker='o')\n",
    "    axs[1].plot(eval_df['epoch'], eval_df['eval_f1'], label='F1-score', marker='o')\n",
    "    axs[1].set_title('Общие метрики на валидации')\n",
    "    axs[1].set_xlabel('Эпоха')\n",
    "    axs[1].set_ylabel('Значение')\n",
    "    axs[1].legend()\n",
    "\n",
    "    # 3. График точности (Accuracy)\n",
    "    axs[2].plot(eval_df['epoch'], eval_df['eval_accuracy'], label='Accuracy', marker='o', color='purple')\n",
    "    axs[2].set_title('Точность на валидации')\n",
    "    axs[2].set_xlabel('Эпоха')\n",
    "    axs[2].set_ylabel('Accuracy')\n",
    "    axs[2].legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Запускаем построение графиков\n",
    "plot_training_history(trainer.state.log_history)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7920084,
     "sourceId": 12544616,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7920621,
     "sourceId": 12545349,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
